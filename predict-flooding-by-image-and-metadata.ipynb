{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":103210,"databundleVersionId":12423451,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================\n# Imports & Setup\n# ============================\nimport os\nimport json\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image, ImageChops\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nimport re\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import AutoTokenizer, AutoModel\nimport torch.nn as nn\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ============================\n# Config\n# ============================\nBATCH_SIZE = 64\nEPOCHS = 15\nLR = 2e-5\nIMG_SIZE = 224\nMAX_LEN = 256\nPATIENCE = 3\n\ntrain_img_path = '/kaggle/input/2025-sum-dpl-302-m/devset_images/devset_images'\ntrain_json_path = '/kaggle/input/2025-sum-dpl-302-m/devset_images_metadata.json'\ntrain_label_path = '/kaggle/input/2025-sum-dpl-302-m/devset_images_gt.csv'\ntest_img_path = '/kaggle/input/2025-sum-dpl-302-m/testset_images/testset_images'\ntest_csv_path = '/kaggle/input/2025-sum-dpl-302-m/test.csv'\nsave_result_path = '/kaggle/working/vit_bert2_predictions.csv'\nsave_checkpoint_path = '/kaggle/working/checkpoint.pth'\n\n# ============================\n# Utility Functions\n# ============================\ndef safe_str(val):\n    return val if isinstance(val, str) else \"\"\n\ndef clean_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower()\n    text = re.sub(r'\\b(img_\\d+|dscf\\d+|[\\w-]+\\.jpg)\\b', '', text)\n    text = re.sub(r'\\b\\d{2,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b', '', text)\n    text = re.sub(r'\\b\\w{5,}\\d+\\b', '', text)\n    text = re.sub(r'[^a-z\\s]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef load_metadata(json_path):\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    return {\n        str(item['image_id']): {\n            'title': safe_str(item.get('title')),\n            'description': safe_str(item.get('description')),\n            'user_tags': \" \".join(item.get('user_tags', [])) if isinstance(item.get('user_tags'), list) else \"\"\n        } for item in data['images']\n    }\n\ndef load_labels(csv_path):\n    df = pd.read_csv(csv_path)\n    return dict(zip(df['id'].astype(str), df['label']))\n\ndef compute_metrics_binary(preds, labels):\n    preds = (preds > 0.5).astype(int)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds),\n        \"precision\": precision_score(labels, preds),\n        \"recall\": recall_score(labels, preds)\n    }\n\ndef autocrop_white_border(image, threshold=245):\n    gray = image.convert(\"L\")\n    bg = Image.new(\"L\", image.size, 255)\n    diff = ImageChops.difference(gray, bg)\n    bbox = diff.getbbox()\n    if bbox:\n        return image.crop(bbox)\n    return image\n\n# ============================\n# Dataset\n# ============================\nclass FloodDataset(Dataset):\n    def __init__(self, image_dir, metadata_dict, label_dict=None, tokenizer=None, transform=None):\n        self.image_dir = Path(image_dir)\n        self.metadata = metadata_dict\n        self.labels = label_dict\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.image_ids = [\n            image_id for image_id in metadata_dict\n            if any((self.image_dir / f\"{image_id}{ext}\").exists() for ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"])\n        ]\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        img_path = next((self.image_dir / f\"{image_id}{ext}\" for ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"] if (self.image_dir / f\"{image_id}{ext}\").exists()), None)\n        image = Image.open(img_path).convert(\"RGB\")\n        image = autocrop_white_border(image)\n        if self.transform:\n            image = self.transform(image)\n\n        text_data = self.metadata[image_id]\n        text_combined = (\n            \"Title: \" + clean_text(text_data.get(\"title\", \"\")) +\n            \" [SEP] Desc: \" + clean_text(text_data.get(\"description\", \"\")) +\n            \" [SEP] Tags: \" + clean_text(text_data.get(\"user_tags\", \"\"))\n        )\n        encoding = self.tokenizer(text_combined, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n\n        if self.labels:\n            label = torch.tensor(self.labels[image_id], dtype=torch.float32)\n            return image, input_ids, attention_mask, label\n        else:\n            return image, input_ids, attention_mask, image_id\n\n# ============================\n# Model\n# ============================\nclass MultiModalDeBERTaViT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vit = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n        self.text_model = AutoModel.from_pretrained(\"microsoft/deberta-v3-small\")\n        self.vit_fc = nn.Sequential(nn.Linear(768, 256), nn.ReLU(), nn.Dropout(0.3))\n        self.text_fc = nn.Sequential(nn.Linear(768, 256), nn.ReLU(), nn.Dropout(0.3))\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, image, input_ids, attention_mask):\n        img_feat = self.vit(pixel_values=image).last_hidden_state[:, 0, :]\n        img_feat = self.vit_fc(img_feat)\n        text_feat = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n        text_feat = self.text_fc(text_feat)\n        combined = torch.cat((img_feat, text_feat), dim=1)\n        return self.classifier(combined)\n\n# ============================\n# Train & Eval\n# ============================\ndef train_epoch(model, dataloader, optimizer, criterion, scaler, scheduler):\n    model.train()\n    total_loss = 0\n    for images, input_ids, attention_mask, labels in dataloader:\n        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.unsqueeze(1).to(device)\n        optimizer.zero_grad()\n        with torch.amp.autocast(device_type='cuda'):\n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    preds, targets, total_loss = [], [], 0\n    with torch.no_grad():\n        for images, input_ids, attention_mask, labels in dataloader:\n            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.unsqueeze(1).to(device)\n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            preds.extend(torch.sigmoid(outputs).cpu().numpy())\n            targets.extend(labels.cpu().numpy())\n    metrics = compute_metrics_binary(np.array(preds).flatten(), np.array(targets).flatten())\n    metrics['loss'] = total_loss / len(dataloader)\n    return metrics\n\n# ============================\n# Main\n# ============================\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.1, 0.1, 0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\nmetadata = load_metadata(train_json_path)\nlabels = load_labels(train_label_path)\ntrain_ids, val_ids = train_test_split(list(labels.keys()), test_size=0.1, random_state=42)\ntrain_ds = FloodDataset(train_img_path, {k: metadata[k] for k in train_ids}, {k: labels[k] for k in train_ids}, tokenizer, transform)\nval_ds = FloodDataset(train_img_path, {k: metadata[k] for k in val_ids}, {k: labels[k] for k in val_ids}, tokenizer, transform)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n\nmodel = MultiModalDeBERTaViT()\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=LR,\n    steps_per_epoch=len(train_loader),\n    epochs=EPOCHS,\n    pct_start=0.1,\n    div_factor=25.0,\n    final_div_factor=1e4,\n    anneal_strategy='cos',\n    verbose=True\n)\n\ncriterion = nn.BCEWithLogitsLoss()\nscaler = GradScaler()\n\nbest_f1, patience = 0, 0\nfor epoch in range(EPOCHS):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, scheduler)\n    val_metrics = evaluate(model, val_loader, criterion)\n    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | F1: {val_metrics['f1']:.4f}\")\n\n    if val_metrics['f1'] > best_f1:\n        best_f1 = val_metrics['f1']\n        patience = 0\n        torch.save(model.state_dict(), save_checkpoint_path)\n    else:\n        patience += 1\n        if patience >= PATIENCE:\n            print(\"Early stopping.\")\n            break\n\n# ============================\n# Inference\n# ============================\nprint(\"Inferencing on test set...\")\nmodel.load_state_dict(torch.load(save_checkpoint_path))\nmodel.eval()\n\ntest_df = pd.read_csv(test_csv_path)\ntest_metadata = {\n    str(row['image_id']): {\n        'title': row['title'] if pd.notna(row['title']) else \"\",\n        'description': row['description'] if pd.notna(row['description']) else \"\",\n        'user_tags': row['user_tags'] if pd.notna(row['user_tags']) else \"\"\n    } for _, row in test_df.iterrows()\n}\ntest_ds = FloodDataset(test_img_path, test_metadata, tokenizer=tokenizer, transform=transform)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n\nresults = []\nwith torch.no_grad():\n    for images, input_ids, attention_mask, image_ids in test_loader:\n        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n        outputs = model(images, input_ids, attention_mask)\n        preds = (torch.sigmoid(outputs).cpu().numpy() > 0.5).astype(int).flatten()\n        results.extend(zip(image_ids, preds))\n\npd.DataFrame(results, columns=[\"id\", \"label\"]).to_csv(save_result_path, index=False)\nprint(\"Saved test predictions to:\", save_result_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T02:25:28.004558Z","iopub.execute_input":"2025-08-05T02:25:28.005086Z","iopub.status.idle":"2025-08-05T02:52:37.622934Z","shell.execute_reply.started":"2025-08-05T02:25:28.005062Z","shell.execute_reply":"2025-08-05T02:52:37.622276Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/tmp/ipykernel_36/631841041.py:232: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.6834 | Val Loss: 0.6400 | F1: 0.0000\nEpoch 2 | Train Loss: 0.4052 | Val Loss: 0.2227 | F1: 0.8983\nEpoch 3 | Train Loss: 0.1836 | Val Loss: 0.1851 | F1: 0.9165\nEpoch 4 | Train Loss: 0.1153 | Val Loss: 0.1276 | F1: 0.9396\nEpoch 5 | Train Loss: 0.0727 | Val Loss: 0.1513 | F1: 0.9247\nEpoch 6 | Train Loss: 0.0505 | Val Loss: 0.1461 | F1: 0.9344\nEpoch 7 | Train Loss: 0.0375 | Val Loss: 0.1290 | F1: 0.9415\nEpoch 8 | Train Loss: 0.0290 | Val Loss: 0.1639 | F1: 0.9291\nEpoch 9 | Train Loss: 0.0214 | Val Loss: 0.1476 | F1: 0.9351\nEpoch 10 | Train Loss: 0.0152 | Val Loss: 0.1420 | F1: 0.9496\nEpoch 11 | Train Loss: 0.0117 | Val Loss: 0.1574 | F1: 0.9424\nEpoch 12 | Train Loss: 0.0113 | Val Loss: 0.1430 | F1: 0.9393\nEpoch 13 | Train Loss: 0.0101 | Val Loss: 0.1530 | F1: 0.9396\nEarly stopping.\nInferencing on test set...\nSaved test predictions to: /kaggle/working/vit_bert2_predictions.csv\n","output_type":"stream"}],"execution_count":2}]}